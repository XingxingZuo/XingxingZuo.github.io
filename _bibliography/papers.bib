---
---
@article{lv2023tmech,
  title={<a name="lnk_lv2023tmech">Continuous-Time Fixed-Lag Smoothing for LiDAR-Inertial-Camera SLAM </a>},
  author={Lv, Jiajun and Lang, Xiaolei and Xu, Jinhong and Wang, Mengmeng and Liu, Yong and <em><b>Zuo<sup>#</sup></b></em>, <em><b>Xingxing</b></em>},
  journal={IEEE/ASME Transactions on Mechatronics},
  year={2023},
  publisher={IEEE},
  abstract={Localization and mapping with heterogeneous multisensor fusion have been prevalent in recent years. To adequately fuse multimodal sensor measurements received at different time instants and different frequencies, we estimate the continuous-time trajectory by fixed-lag smoothing within a factor-graph optimization framework. With the continuous-time formulation, we can query poses at any time instants corresponding to the sensor measurements. To bound the computation complexity of the continuous-time fixed-lag smoother, we maintain temporal and keyframe sliding windows with constant size, and probabilistically marginalize out control points of the trajectory and other states, which allows preserving prior information for future sliding-window optimization. Based on continuous-time fixed-lag smoothing, we design tightly coupled multimodal SLAM algorithms with a variety of sensor combinations, like the LiDAR-inertial and LiDAR-inertial-camera SLAM systems, in which online time offset calibration is also naturally supported. More importantly, benefiting from the marginalization and our derived analytical Jacobians for optimization, the proposed continuous-time SLAM systems can achieve real-time performance regardless of the high complexity of continuous-time formulation. The proposed multimodal SLAM systems have been widely evaluated on three public datasets and self-collect datasets. The results demonstrate that the proposed continuous-time SLAM systems can achieve high-accuracy pose estimations and outperform existing state-of-the-art methods.},
  abbr={clic},
  arxiv={2302.07456},
  code={https://github.com/APRIL-ZJU/clic}
}

@article{lang2022ral,
  title={<a name="lnk_lang2022ral">Ctrl-VIO: Continuous-Time Visual-Inertial Odometry for Rolling Shutter Cameras </a>},
  author={Lang, Xiaolei and Lv, Jiajun and Huang, Jianxin and Ma, Yukai and Liu, Yong and <em><b>Zuo<sup>#</sup></b></em>, <em><b>Xingxing</b></em>},
  journal={ IEEE Robotics and Automation Letters},
  year={2022},
  publisher={IEEE},
  abstract={In this letter, we propose a probabilistic continuous-time visual-inertial odometry (VIO) for rolling shutter cameras. The continuous-time trajectory formulation naturally facilitates the fusion of asynchronized high-frequency IMU data and motion-distorted rolling shutter images. To prevent intractable computation load, the proposed VIO is sliding-window and keyframe-based. We propose to probabilistically marginalize the control points to keep the constant number of keyframes in the sliding window. Furthermore, the line exposure time difference (line delay) of the rolling shutter camera can be online calibrated in our continuous-time VIO. To extensively examine the performance of our continuous-time VIO, experiments are conducted on publicly-available WHU-RSVI, TUM-RSVI, and SenseTime-RSVI rolling shutter datasets. The results demonstrate the proposed continuous-time VIO significantly outperforms the existing state-of-the-art VIO methods.},
  abbr={ctrl_vio},
  arxiv={2208.12008},
  code={https://github.com/APRIL-ZJU/Ctrl-VIO}
}

@article{lv2022tro,
  title={<a name="lnk_lv2022tro">Observability-Aware Intrinsic and Extrinsic Calibration of LiDAR-IMU Systems </a>},
  author={Lv*, Jiajun and <em><b>Zuo*</b></em>, <em><b>Xingxing</b></em> and Hu, Kewei and Xu, Jinhong and Huang, Guoquan and Liu, Yong},
  journal={IEEE Transactions on Robotics},
  year={2022},
  publisher={IEEE},
  abstract={Accurate and reliable sensor calibration is essential to fuse LiDAR and inertial measurements, which are usually available in robotic applications. In this paper, we propose a novel LiDAR-IMU calibration method within the continuous-time batch-optimization framework, where the intrinsics of both sensors and the spatial-temporal extrinsics between sensors are calibrated without using calibration infrastructure such as fiducial tags. Compared to discrete-time approaches, the continuous-time formulation has natural advantages for fusing high rate measurements from LiDAR and IMU sensors. To improve efficiency and address degenerate motions, two observability-aware modules are leveraged: (i) The information-theoretic data selection policy selects only the most informative segments for calibration during data collection, which significantly improves the calibration efficiency by processing only the selected informative segments. (ii) The observability-aware state update mechanism in nonlinear least-squares optimization updates only the identifiable directions in the state space with truncated singular value decomposition (TSVD), which enables accurate calibration results even under degenerate cases where informative data segments are not available. The proposed LiDAR-IMU calibration approach has been validated extensively in both simulated and real-world experiments with different robot platforms, demonstrating its high accuracy and repeatability in commonly-seen human-made environments.},
  abbr={ob_aware_calib},
  arxiv={2205.03276},
  code={https://github.com/APRIL-ZJU/OA-LICalib}
}

@InProceedings{nate2022cvpr,
  title={<a name="lnk_nate2022cvpr">Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation</a>},
  author={Merrill, Nathaniel  and Guo, Yuliang and Zuo, Xingxing and Huang, Xinyu and Leutenegger, Stefan and Peng, Xi and Ren, Liu and Huang, Guoquan},
  Booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  abstract={We propose a keypoint-based object-level SLAM framework that can provide globally consistent 6DoF pose estimates for symmetric and asymmetric objects alike. To the best of our knowledge, our system is among the first to utilize the camera pose information from SLAM to provide prior knowledge for tracking keypoints on symmetric objects -- ensuring that new measurements are consistent with the current 3D scene. Moreover, our semantic keypoint network is trained to predict the Gaussian covariance for the keypoints that captures the true error of the prediction, and thus is not only useful as a weight for the residuals in the system's optimization problems, but also as a means to detect harmful statistical outliers without choosing a manual threshold. Experiments show that our method provides competitive performance to the state of the art in 6DoF object pose estimation, and at a real-time speed. Our code, pre-trained models, and keypoint labels are open-sourced.},
  abbr={suo_slam},
  arxiv={2205.01823},
  code={https://github.com/rpng/suo_slam}
}

@inproceedings{zuo2021codevio,
  title={<a name="lnk_codevio"> CodeVIO: Visual-inertial odometry with learned optimizable dense depth </a>},
  author={Zuo, Xingxing and Merrill, Nathaniel and Li, Wei and Liu, Yong and Pollefeys, Marc and Huang, Guoquan},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  comments={<b style="color:red;">Nominated for the "Best Paper Award in Robot Vision (Finalist) of ICRA 2021"</b>},
  pages={14382--14388},
  year={2021},
  organization={IEEE},
  abstract={In this work, we present a lightweight, tightly-coupled deep depth network and visual-inertial odometry (VIO) system, which can provide accurate state estimates and dense depth maps of the immediate surroundings. Leveraging the proposed lightweight Conditional Variational Autoencoder (CVAE) for depth inference and encoding, we provide the network with previously marginalized sparse features from VIO to increase the accuracy of initial depth prediction and generalization capability. The compact encoded depth maps are then updated jointly with navigation states in a sliding window estimator in order to provide the dense local scene geometry. We additionally propose a novel method to obtain the CVAE's Jacobian which is shown to be more than an order of magnitude faster than previous works, and we additionally leverage First-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous works relying on completely dense residuals, we propose to only provide sparse measurements to update the depth code and show through careful experimentation that our choice of sparse measurements and FEJs can still significantly improve the estimated depth maps. Our full system also exhibits state-of-the-art pose estimation accuracy, and we show that it can run in real-time with single-thread execution while utilizing GPU acceleration only for the network and code Jacobian.},
  abbr={codevio},
  arxiv={2012.10133},
  video={https://www.youtube.com/watch?v=tr_vflFjghY}
}

@article{zhang2021pose,
  title={Pose Estimation for Ground Robots: On Manifold Representation, Integration, Reparameterization, and Optimization},
  author={Zhang*, Mingming and <em><b>Zuo*</b></em>, <em><b>Xingxing</b></em> and Chen, Yiming and Liu, Yong and Li, Mingyang},
  journal={IEEE Transactions on Robotics},
  year={2021},
  volume={37},
  number={4},
  pages={1081-1099},
  publisher={IEEE},
  abstract={In this article, we focus on pose estimation dedicated to nonholonomic ground robots with low-cost sensors, by probabilistically fusing measurements from wheel odometers and an exteroceptive sensor. For ground robots, wheel odometers are widely used in pose estimation tasks, especially in applications in planar scenes. However, since wheel odometer only provides two-dimensional (2D) motion measurements, it is extremely challenging to use that for accurate full 6-D pose (3-D position and 3-D orientation) estimation. Traditional methods for 6-D pose estimation with wheel odometers either approximate motion profiles at the cost of accuracy reduction, or rely on other sensors, e.g., inertial measurement unit, to provide complementary measurements. By contrast, we propose a novel motion-manifold-based method for pose estimation of ground robots, which enables to utilize wheel odometers for high-precision 6-D pose estimation. Specifically, the proposed method, first, formulates the motion manifold of ground robots by a parametric representation, second, performs manifold-based 6-D integration with wheel odometer measurements only, and third, reparameterizes manifold representation periodically for error reduction. To demonstrate the effectiveness and applicability of the proposed algorithmic module, we integrate that into a sliding-window pose estimator by using measurements from wheel odometers and a monocular camera. Extensive simulated and real-world experiments are conducted for evaluation, and the proposed algorithm is shown to outperform competing the state-of-the-art algorithms by a significant margin in pose estimation accuracy, especially when deployed in complex, large-scale real-world environments.},
  abbr={manifold},
  arxiv={1909.03423},
  video={https://www.youtube.com/watch?v=XGsQPqx2LY4}
}

@InProceedings{lv2021clins,
  title={CLINS: Continuous-Time Trajectory Estimation for LiDAR-Inertial System},
  author={Lv, Jiajun and Hu, Kewei and Xu, Jinhong and Liu, Yong and Ma, Xiushui and Zuo, Xingxing},
  Booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2021},
  abstract={In this paper, we propose a highly accurate continuous-time trajectory estimation framework dedicated to SLAM (Simultaneous Localization and Mapping) applications, which enables fuse high-frequency and asynchronous sensor data effectively. We apply the proposed framework in a 3D LiDAR-inertial system for evaluations. The proposed method adopts a non-rigid registration method for continuous-time trajectory estimation and simultaneously removing the motion distortion in LiDAR scans. Additionally, we propose a two-state continuous-time trajectory correction method to efficiently and efficiently tackle the computationally-intractable global optimization problem when loop closure happens. We examine the accuracy of the proposed approach on several publicly available datasets and the data we collected. The experimental results indicate that the proposed method outperforms the discrete-time methods regarding accuracy especially when aggressive motion occurs. Furthermore, we open source our code to benefit research community.},
  abbr={clins},
  arxiv={2109.04687},
  video={https://www.youtube.com/watch?v=7aQJklHg2RM},
  code={https://github.com/APRIL-ZJU/clins}
}


@InProceedings{liu2021mba,
  title={MBA-VO: Motion Blur Aware Visual Odometry},
  author={Liu, Peidong and Zuo, Xingxing and Larsson, Viktor and Pollefeys, Marc},
  Booktitle = {International Conference on Computer Vision (ICCV), selected as <b>Oral</b>, },
  year={2021},
  abstract={Motion blur is one of the major challenges remaining for visual odometry methods. In low-light conditions where longer exposure times are necessary, motion blur can appear even for relatively slow camera motions. In this paper we present a novel hybrid visual odometry pipeline with direct approach that explicitly models and estimates the camera's local trajectory within the exposure time. This allows us to actively compensate for any motion blur that occurs due to the camera motion. In addition, we also contribute a novel benchmarking dataset for motion blur aware visual odometry. In experiments we show that by directly modeling the image formation process, we are able to improve robustness of the visual odometry, while keeping comparable accuracy as that for images without motion blur.},
  abbr={mba_vo},
  arxiv={2103.13684},
  code={https://github.com/ethliup/MBA-VO}
}

@InProceedings{Zuo2020IROS,
  Title                    = {LIC-Fusion 2.0: LiDAR-Inertial-Camera Odometry with Sliding-Window Plane-Feature Tracking},
  author={Zuo, Xingxing and Yang, Yulin and Geneva, Patrick and Lv, Jiajun and Liu, Yong and Huang, Guoquan and Pollefeys, Marc},
  Booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2020},
  pages={5112-5119},
  Address                  = {Las Vegas, NV, USA},
  abstract={Multi-sensor fusion of multi-modal measurements from commodity inertial, visual and LiDAR sensors to provide robust and accurate 6DOF pose estimation holds great potential in robotics and beyond. In this paper, building upon our prior work (i.e., LIC-Fusion), we develop a sliding-window filter based LiDAR-Inertial-Camera odometry with online spatiotemporal calibration (i.e., LIC-Fusion 2.0), which introduces a novel sliding-window plane-feature tracking for efficiently processing 3D LiDAR point clouds. In particular, after motion compensation for LiDAR points by leveraging IMU data, low-curvature planar points are extracted and tracked across the sliding window. A novel outlier rejection criterion is proposed in the plane-feature tracking for high-quality data association. Only the tracked planar points belonging to the same plane will be used for plane initialization, which makes the plane extraction efficient and robust. Moreover, we perform the observability analysis for the LiDAR-IMU subsystem and report the degenerate cases for spatiotemporal calibration using plane features. While the estimation consistency and identified degenerate motions are validated in Monte-Carlo simulations, different real-world experiments are also conducted to show that the proposed LIC-Fusion 2.0 outperforms its predecessor and other state-of-the-art methods.},
  supp={tr_lic2.pdf},
  abbr={lic2_sim},
  arxiv={2008.07196},
  video={https://www.youtube.com/watch?v=waE5nepxD-Q},
  video_extra={https://www.youtube.com/watch?v=waE5nepxD-Q}
}


@article{zuo2020multimodal,
  title={Multimodal localization: Stereo over LiDAR map},
  author={Zuo, Xingxing and Ye, Wenlong and Yang, Yulin and Zheng, Renjie and Vidal-Calleja, Teresa and Huang, Guoquan and Liu, Yong},
  journal={Journal of Field Robotics},
  volume={37},
  number={6},
  pages={1003--1026},
  year={2020},
  publisher={Wiley Online Library},
  abstract={In this paper, we present a real-time high-precision visual localization system for an autonomous vehicle which employs only low-cost stereo cameras to localize the vehicle with a priori map built using a more expensive 3D LiDAR sensor. To this end, we construct two different visual maps: a sparse feature visual map for visual odometry (VO) based motion tracking, and a semidense visual map for registration with the prior LiDAR map. To register two point clouds sourced from different modalities (i.e., cameras and LiDAR), we leverage probabilistic weighted normal distributions transformation (ProW-NDT), by particularly taking into account the uncertainty of source point clouds. The registration results are then fused via pose graph optimization to correct the VO drift. Moreover, surfels extracted from the prior LiDAR map are used to refine the sparse 3D visual features that will further improve VO-based motion estimation. The proposed system has been tested extensively in both simulated and real-world experiments, showing that robust, high-precision, real-time localization can be achieved.},
  abbr={jfr_multimodal}
}

@inproceedings{lv2020targetless,
  title={Targetless calibration of lidar-imu system based on continuous-time batch estimation},
  author={Lv, Jiajun and Xu, Jinhong and Hu, Kewei and Liu, Yong and Zuo, Xingxing},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={9968--9975},
  year={2020},
  organization={IEEE},
  abstract={Sensor calibration is the fundamental block for a multi-sensor fusion system. This paper presents an accurate and repeatable LiDAR-IMU calibration method (termed LI-Calib), to calibrate the 6-DOF extrinsic transformation between the 3D LiDAR and the Inertial Measurement Unit (IMU). % Regarding the high data capture rate for LiDAR and IMU sensors, LI-Calib adopts a continuous-time trajectory formulation based on B-Spline, which is more suitable for fusing high-rate or asynchronous measurements than discrete-time based approaches. % Additionally, LI-Calib decomposes the space into cells and identifies the planar segments for data association, which renders the calibration problem well-constrained in usual scenarios without any artificial targets. We validate the proposed calibration approach on both simulated and real-world experiments. The results demonstrate the high accuracy and good repeatability of the proposed method in common human-made scenarios. To benefit the research community, the code of this useful toolbox is also open-source.},
  abbr={LIcalib},
  arxiv={2007.14759},
  video={https://www.youtube.com/watch?v=umXgc4bfhy0},
  code={https://github.com/APRIL-ZJU/lidar_IMU_calib}
}

@InProceedings{Zuo2019IROS,
  Title                    = {LIC-Fusion: LiDAR-Inertial-Camera Odometry},
  Author                   = {Zuo, Xingxing and Geneva, Patrick and Lee, Woosik and Liu, Yong and Huang, Guoquan},
  Booktitle                = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
    year={2019},
  pages={5848-5854},
  doi={10.1109/IROS40897.2019.8967746},
  Address                  = {Macau, China},
  abstract={This paper presents a tightly-coupled multi-sensor fusion algorithm termed LiDAR-inertial-camera fusion (LIC-Fusion), which efficiently fuses IMU measurements, sparse visual features, and extracted LiDAR points. In particular, the proposed LIC-Fusion performs online spatial and temporal sensor calibration between all three asynchronous sensors, in order to compensate for possible calibration variations. The key contribution is the optimal (up to linearization errors) multi-modal sensor fusion of detected and tracked sparse edge/surf feature points from LiDAR scans within an efficient MSCKF-based framework, alongside sparse visual feature observations and IMU readings. We perform extensive experiments in both indoor and outdoor environments, showing that the proposed LIC-Fusion outperforms the state-of-the-art visual-inertial odometry (VIO) and LiDAR odometry methods in terms of estimation accuracy and robustness to aggressive motions.},
  abbr={lic1},
  arxiv={1909.04102},
  video={https://www.youtube.com/watch?v=z8WG4miBk3s}
}

@InProceedings{Zuo2019ISRR,
  Title                    = {Visual-Inertial Localization for Skid-Steering Robots with Kinematic Constraints},
  Author                   = {Zuo, Xingxing and Zhang, Mingming and Chen, Yiming and Liu, Yong and Huang, Guoquan and Li, Mingyang},
  Booktitle                = {International Symposium on Robotics Research (ISRR)},
  Year                     = {2019},
  Address                  = {Hanoi, Vietnam},
  abstract={While visual localization or SLAM has witnessed great progress in past decades, when deploying it on a mobile robot in practice, few works have explicitly considered the kinematic (or dynamic) constraints of the real robotic system when designing state estimators. To promote the practical deployment of current state-of-the-art visual-inertial localization algorithms, in this work we propose a low-cost kinematics-constrained localization system particularly for a skid-steering mobile robot. In particular, we derive in a principle way the robot's kinematic constraints based on the instantaneous centers of rotation (ICR) model and integrate them in a tightly-coupled manner into the sliding-window bundle adjustment (BA)-based visual-inertial estimator. Because the ICR model parameters are time-varying due to, for example, track-to-terrain interaction and terrain roughness, we estimate these kinematic parameters online along with the navigation state. To this end, we perform in-depth the observability analysis and identify motion conditions under which the state/parameter estimation is viable. The proposed kinematics-constrained visual-inertial localization system has been validated extensively in different terrain scenarios.},
  supp={tr_icr.pdf},
  abbr={icr},
  arxiv={1911.05787}
}

@article{zuo2019visual,
  title={Visual-Inertial Localization With Prior LiDAR Map Constraints},
  author={Zuo, Xingxing and Geneva, Patrick and Yang, Yulin and Ye, Wenlong and Liu, Yong and Huang, Guoquan},
  journal={IEEE Robotics and Automation Letters},
  volume={4},
  number={4},
  pages={3394--3401},
  year={2019},
  publisher={IEEE},
  abstract={In this letter, we develop a low-cost stereo visual-inertial localization system, which leverages efficient multi-state constraint Kalman filter (MSCKF)-based visual-inertial odometry (VIO) while utilizing an a priori LiDAR map to provide bounded-error three-dimensional navigation. Besides the standard sparse visual feature measurements used in VIO, the global registrations of visual semi-dense clouds to the prior LiDAR map are also exploited in a tightly-coupled MSCKF update, thus correcting accumulated drift. This cross-modality constraint between visual and LiDAR pointclouds is particularly addressed. The proposed approach is validated on both Monte Carlo simulations and real-world experiments, showing that LiDAR map constraints between clouds created through different sensing modalities greatly improve the standard VIO and provide bounded-error performance.},
  abbr={mapreuse},
  video={https://www.youtube.com/watch?v=76fXMVhhX4E&t=1s}
}


@inproceedings{zuo2017robust,
  title={Robust visual SLAM with point and line features},
  author={Zuo, Xingxing and Xie, Xiaojia and Liu, Yong and Huang, Guoquan},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1775--1782},
  year={2017},
  organization={IEEE},
  abstract={In this paper, we develop a robust efficient visual SLAM system that utilizes heterogeneous point and line features. By leveraging ORB-SLAM [1], the proposed system consists of stereo matching, frame tracking, local mapping, loop detection, and bundle adjustment of both point and line features. In particular, as the main theoretical contributions of this paper, we, for the first time, employ the orthonormal representation as the minimal parameterization to model line features along with point features in visual SLAM and analytically derive the Jacobians of the re-projection errors with respect to the line parameters, which significantly improves the SLAM solution. The proposed SLAM has been extensively tested in both synthetic and real-world experiments whose results demonstrate that the proposed system outperforms the state-of-the-art methods in various scenarios.},
  abbr={plslam},
  arxiv={1711.08654},
  video={https://www.youtube.com/watch?v=qK8vgY1Gjgg}
}