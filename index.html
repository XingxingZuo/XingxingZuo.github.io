<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Xingxing Zuo


</title>
<meta name="description" content="Xingxing Zuo's personal webpage.
">

<!-- Open Graph -->

<meta property="og:site_name" content="Xingxing Zuo's personal webpage.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="https://XingxingZuo.github.io/" />
<meta property="og:description" content="about" />
<meta property="og:image" content="https://twitter.com/12Zuo" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>✨</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L6PYDPEBZZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-L6PYDPEBZZ');
</script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%78%69%6E%67%78%69%6E%67.%7A%75%6F@%6D%62%7A%75%61%69.%61%63.%61%65"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=CePv8agAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/ZuoJiaxing" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/xingxingzuo" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/12Zuo" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>











<!--  -->

        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              home
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="https://XingxingZuo.github.io/#publications">publications</a>
          </li>
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/people/">
                people
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/workshop/">
                workshop
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Xingxing Zuo
    </h1>
     <p class="desc">Assistant Professor at <a href="https://mbzuai.ac.ae/" target="_blank" rel="noopener noreferrer"> MBZUAI</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/xingxingzuo.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I’m an Assistant Professor (tenure-track) in the Robotics Department at <a href="https://mbzuai.ac.ae/" target="_blank" rel="noopener noreferrer">MBZUAI</a>, where I lead the Robotics Cognition and Learning (RCL) Group. Prior to this, I was a Postdoc in the Department of Computing and Mathematical Sciences at <a href="https://www.cms.caltech.edu/" target="_blank" rel="noopener noreferrer">Caltech</a>, working with Prof. <a href="https://www.eas.caltech.edu/people/sjchung" target="_blank" rel="noopener noreferrer">Soon-Jo Chung</a>. In 2023, I worked as a full-time Visiting Faculty Researcher (Scientist) at Google in the United States. From 2021 to 2023, I was a Postdoctoral Researcher at the <a href="https://www.tum.de/en" target="_blank" rel="noopener noreferrer">Technical University of Munich</a> in Germany, working with Prof. <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>. Between 2019 and 2021, I was an academic guest in the Department of Computer Science at <a href="https://ethz.ch/en.html" target="_blank" rel="noopener noreferrer">ETH Zurich</a> in Switzerland, working with Prof. <a href="https://people.inf.ethz.ch/marc.pollefeys/" target="_blank" rel="noopener noreferrer">Marc Pollefeys</a>. I also held visiting scholar positions at the <a href="https://www.udel.edu/" target="_blank" rel="noopener noreferrer">University of Delaware</a>, USA (2018), and the <a href="https://www.uts.edu.au/" target="_blank" rel="noopener noreferrer">University of Technology Sydney</a>, Australia (2017). I received my Ph.D. in 2021 from <a href="https://www.zju.edu.cn/english/" target="_blank" rel="noopener noreferrer">Zhejiang University</a> in China, graduating with honors under the supervision of Prof. <a href="https://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan (Paul) Huang</a> and Prof. Yong Liu. I earned my Bachelor’s degree also with honors, from the <a href="https://en.uestc.edu.cn/" target="_blank" rel="noopener noreferrer">University of Electronic Science and Technology of China</a> (UESTC), Chengdu, China, in 2016.</p>

<p>I am <strong>hiring</strong> highly motivated PhD students, research assistants, visiting graduate students and Postdocs. Please check this <a href="assets/pdf/hiring2026_En.pdf">page</a> (or <a href="https://zhuanlan.zhihu.com/p/1890568428944867704" target="_blank" rel="noopener noreferrer">Chinese version</a>) for more details! For application, please send an email to me via <a href="mailto:xingxingzuo@outlook.com">xingxingzuo[at]outlook.com</a> with the subject “[Position]+[Name]+[Affiliation]”!</p>

<div class="row  align-items-center">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid " src="/assets/img/affiliations/compound.png" alt="" title="example image">
    </div>
</div>

<hr>

<h2>research interests</h2>
<p>My research interests include robot perception, 3D computer vision, visual-inertial learning, state estimation, human-object interaction,  mobile manipulation, VLN/VLA, spatial AI and embodied AI.</p>

<p>My long-term research vision is to enable natural interaction and seamless collaboration between robots and humans in open environments by accurately understanding robot states, surrounding 3D scenes, and action execution.</p>

<hr>

<div class="news">

  
    <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row" style="width:15%">Jan 12, 2026</th>
          <td>
            
              Our work about <a href="https://xingxingzuo.github.io/flying_co_stereo" target="_blank" rel="noopener noreferrer">Flying Co-Stereo</a> is accepted by T-RO!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jan 11, 2026</th>
          <td>
            
              Our workshop proposal for <a href="https://xingxingzuo.github.io/MM-SpatialAI/" target="_blank" rel="noopener noreferrer">Multi-Modal Spatial AI</a> is accepted by ICRA 2026. See you in Vienna!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Nov 16, 2025</th>
          <td>
            
              Thrilled to be appointed as an Area Chair for the top conference <a href="https://roboticsconference.org/" target="_blank" rel="noopener noreferrer">RSS 2026</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Oct 8, 2025</th>
          <td>
            
              Thrilled to be appointed as an Associate Editor for the prestigious journal <a href="https://www.ieee-ras.org/publications/t-ro" target="_blank" rel="noopener noreferrer">T-RO</a>!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Sep 12, 2025</th>
          <td>
            
              Serving as an Associate Editor for <a href="https://2026.ieee-icra.org/" target="_blank" rel="noopener noreferrer">ICRA 2026</a>,  my fourth consecutive year handling ICRA submissions!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Sep 3, 2025</th>
          <td>
            
              “<a href="https://arxiv.org/abs/2411.15800" target="_blank" rel="noopener noreferrer">PG-SLAM</a>” about dynamic SLAM is accepted by T-RO!


            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jul 9, 2025</th>
          <td>
            
              Our work “<a href="https://xingxingzuo.github.io/gaussian_lic2" target="_blank" rel="noopener noreferrer">Gaussian_LIC2</a>” is unveiled on Arxiv!


            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jun 27, 2025</th>
          <td>
            
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=11112654" target="_blank" rel="noopener noreferrer">ROEVO</a> about edge feature-based SLAM is accepted by T-RO!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">May 31, 2025</th>
          <td>
            
              Our work “<a href="https://xingxingzuo.github.io/flying_co_stereo" target="_blank" rel="noopener noreferrer">Flying Co-Stereo</a>” is unveiled on Arxiv!


            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">May 1, 2025</th>
          <td>
            
              Excited to join the Robotics Department at <a href="https://mbzuai.ac.ae/" target="_blank" rel="noopener noreferrer">MBZUAI</a> to start my faculty adventure!


            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jan 27, 2025</th>
          <td>
            
              <a href="https://xingxingzuo.github.io/gaussian_lic/" target="_blank" rel="noopener noreferrer">Gaussian-LIC</a> is accepted by ICRA 2025!


            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jan 15, 2025</th>
          <td>
            
              Excited to give a talk about robotic perception at MIT! Many thanks to Prof. Luca Carlone’s group for hosting me!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jan 8, 2025</th>
          <td>
            
              Happy to announce that we are going to organize a workshop “<a href="https://sites.google.com/view/tiro25/" target="_blank" rel="noopener noreferrer">Thermal Infrared  in Robotics</a>” at ICRA 2025!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Jan 6, 2025</th>
          <td>
            
              Our paper about thermal depth prediction is accepted by RA-L!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row" style="width:15%">Sep 10, 2024</th>
          <td>
            
              Serving as an Associate Editor for <a href="https://2025.ieee-icra.org/" target="_blank" rel="noopener noreferrer">ICRA 2025</a>,  my third consecutive year handling ICRA submissions!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

  

</div>

<hr>

<h2>academic services</h2>
<p><b>Journal Associate Editor</b>: IEEE Transactions on Robotics (<a href="https://www.ieee-ras.org/publications/t-ro" target="_blank" rel="noopener noreferrer">T-RO</a>, from January 2026), IEEE Robotics and Automation Letters (<a href="https://www.ieee-ras.org/publications/ra-l" target="_blank" rel="noopener noreferrer">RA-L</a>, since March 2022)</p>

<p><b>Conference Associate Editor / Area Chair</b>:  Robotics: Science and Systems (<a href="https://roboticsconference.org/" target="_blank" rel="noopener noreferrer">RSS</a>, 2026), IEEE/RSJ International Conference on Intelligent Robots and Systems (<a href="https://www.ieee-ras.org/conferences-workshops/financially-co-sponsored/iros" target="_blank" rel="noopener noreferrer">IROS</a>, 2022&amp;2023&amp;2024&amp;2025), IEEE International Conference on Robotics and Automation (<a href="https://www.ieee-ras.org/conferences-workshops/fully-sponsored/icra" target="_blank" rel="noopener noreferrer">ICRA</a>, 2023&amp;2024&amp;2025&amp;2026).</p>

<p><b>Workshop Organizer</b>:  <a href="https://sites.google.com/view/tiro25/" target="_blank" rel="noopener noreferrer">Thermal Infrared in Robotics</a> in ICRA 2025.</p>

<p><b>Program Committee / Area Chair</b>:  <a href="https://ieeemobility.org/MOST2025/" target="_blank" rel="noopener noreferrer">IEEE MOST 2025</a>; <a href="https://sairlab.org/icra25/" target="_blank" rel="noopener noreferrer">ICRA 2025 Workshop on Foundation Models and Neuro-Symbolic AI for Robotics</a></p>

<p><b>Conference Session Chair</b>: IROS 2023.</p>

<p><b>Guest Editor</b>: Sensors [<a href="https://www.mdpi.com/journal/sensors/special_issues/efficient_embedded_sensing" target="_blank" rel="noopener noreferrer">Special Issue 2023</a>].</p>

<p><b>Journal Reviewer</b>: IEEE Transactions on Robotics (T-RO), The International Journal of
Robotics Research (IJRR), Journal of Field Robotics (J-FR), IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), IEEE Transactions on Automation Science and Engineering (T-ASE), IEEE Robotics and Automation Letters (RA-L), IEEE Transactions on Intelligent Transportation Systems (T-ITS), IEEE Transactions on Industrial Electronics (T-IE), Neurocomputing.</p>

<p><b>Conference Reviewer</b>: ICRA, IROS, CVPR, ICCV, ECCV, IJCAI</p>

<hr>

<h2>representative talks</h2>
<ul>
  <li>2025.03 A talk about “Spatial Intelligent Robots”. Virginia Tech (Blacksburg).</li>
  <li>2025.03 A talk about “Spatial Intelligent Robots”. KAUST (Saudi Arabia).</li>
  <li>2025.01 A talk about “Robust 3D Perception”. MIT (Boston).</li>
  <li>2023.08 A talk about “Robust 3D Perception”. The Chinese University of Hong Kong (ShenZhen).</li>
  <li>2023.05 A talk about “Robust 3D Perception”. Upenn Grasp Lab.</li>
  <li>2022.06 “Robust 3D Perception with Multi-source Information Fusion”. ETH CVG group.</li>
  <li>2022.03 Invited speaker at conference, “Towards the Era of Robust Pose Estimation for Robots”. UPLINLIBS 2022.</li>
  <li>2022.03 “Towards the Era of Robust Spatial Perception”. Peking University.</li>
</ul>

<hr>

    </div>

    
    
    <div class="clearfix">
    <a name="publications"><h2>selected publications</h2></a>
    <br>
    <p style="font-size:22px;"> <i> Find out my full publication list via </i> <a href="https://scholar.google.com/citations?user=CePv8agAAAAJ" target="_blank" rel="noopener noreferrer">Google Scholar</a>. <br>
    <sup>*</sup> co-first author with equal contribution;  <sup>#</sup> Corresponding author.</p>  
    <!-- <sup>&#x0266F;</sup>corresponding author. -->
    
      <ol class="bibliography"><li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zeng2026flowhoi" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/zeng2026flowhoi1.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lin2026flowhoi"> FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Huajian Zeng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lingyun Chen,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Jiaqi Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuantai Zhang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Fan Shi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Peidong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint</em>
      
      
        
          2026
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
    
      [<a href="https://www.youtube.com/watch?v=c_ayPvccyi8" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
      [<a href="https://huajian-zeng.github.io/projects/flowhoi" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Recent vision-language-action (VLA) models can generate plausible end-effector motions, yet they often fail in long-horizon, contact-rich tasks because the underlying hand-object interaction (HOI) structure is not explicitly represented. An embodiment-agnostic interaction representation that captures this structure would make manipulation behaviors easier to validate and transfer across robots.We propose FlowHOI, a two-stage flow-matching framework that generates semantically grounded, temporally coherent HOI sequences, comprising hand poses, object poses, and hand-object contact states, conditioned on an egocentric observation, a language instruction, and a 3D Gaussian splatting (3DGS) scene reconstruction. We decouple geometry-centric grasping from semantics-centric manipulation, conditioning the latter on compact 3D scene tokens and a motion–text alignment loss to semantically ground the generated interactions in both the physical scene layout and the language instruction. To address the scarcity of high-fidelity HOI supervision, we introduce a reconstruction pipeline that recovers aligned hand-object trajectories and meshes from large-scale egocentric videos, yielding an HOI prior for robust generation. Across the GRAB and HOT3D benchmarks, FlowHOI achieves the highest action recognition accuracy and a 1.7× higher physics simulation success rate than the strongest diffusion-based baseline, while delivering a 40× inference speedup. We further demonstrate real-robot execution on four dexterous manipulation tasks, illustrating the feasibility of retargeting generated HOI representations to real-robot execution pipelines.</p>
    </span>
    
  </div>
</div>
</li></ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lin2025simgen_hoi" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2025simgen_hoi.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lin2025simgen_hoi"> SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction
via Generative Modeling and Reinforcement Learning </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Yuhang Lin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yijia Xie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiahong Xie,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Yuehao Huang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Ruoyu Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yukai Ma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2508.14120</em>
      
      
        
          2025
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2508.14120" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
      [<a href="https://xingxingzuo.github.io/simgen_hoi" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Generating physically realistic humanoid–object interactions (HOI) is a fundamental challenge in robotics. Existing HOI generation approaches, such as diffusion-based models, often suffer from artifacts such as implausible contacts, penetrations, and unrealistic whole-body actions, which hinder successful execution in physical environments. To address these challenges, we introduce SimGenHOI , a unified framework that combines the strengths of generative modeling and reinforcement learning to produce controllable and physically plausible HOI. Our HOI generative model, based on Diffusion Transformers (DiT), predicts a set of key actions conditioned on text prompts, object geometry, sparse object waypoints, and the initial humanoid pose. These key actions capture essential interaction dynamics and are interpolated into smooth motion trajectories, naturally supporting long-horizon generation. To ensure physical realism, we design a contact-aware whole-body control policy trained with reinforcement learning, which tracks the generated motions while correcting artifacts such as penetration and foot sliding. Furthermore, we introduce a mutual fine-tuning strategy, where the generative model and the control policy iteratively refine each other, improving both motion realism and tracking robustness. Extensive experiments demonstrate that SimGenHOI generates realistic, diverse, and physically plausible humanoid–object interactions, achieving significantly higher tracking success rates in simulation and enabling long-horizon manipulation tasks.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lang2025gaussian" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2025gaussian_lic2.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lang2025gaussian"> Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Xiaolei Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kai Tang,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Laijian Li,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Jianxin Huang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Lina Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu<sup>#</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2507.04004</em>
      
      
        
          2025
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2507.04004" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
      [<a href="https://xingxingzuo.github.io/gaussian_lic2" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/Gaussian-LIC" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents the first photo-realistic LiDAR-Inertial-Camera Gaussian Splatting SLAM system that simultaneously addresses visual quality, geometric accuracy, and real-time performance. The proposed method performs robust and accurate pose estimation within a continuous-time trajectory optimization framework, while incrementally reconstructing a 3D Gaussian map using camera and LiDAR data, all in real time. The resulting map enables high-quality, real-time novel view rendering of both RGB images and depth maps. To effectively address under-reconstruction in regions not covered by the LiDAR, we employ a lightweight zero-shot depth model that synergistically combines RGB appearance cues with sparse LiDAR measurements to generate dense depth maps. The depth completion enables reliable Gaussian initialization in LiDAR-blind areas, significantly improving system applicability for sparse LiDAR sensors. To enhance geometric accuracy, we use sparse but precise LiDAR depths to supervise Gaussian map optimization and accelerate it with carefully designed CUDA-accelerated strategies. Furthermore, we explore how the incrementally reconstructed Gaussian map can improve the robustness of odometry. By tightly incorporating photometric constraints from the Gaussian map into the continuous-time factor graph optimization, we demonstrate improved pose estimation under LiDAR degradation scenarios. We also showcase downstream applications via extending our elaborate system, including video frame interpolation and fast 3D mesh extraction. To support rigorous evaluation, we construct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth poses, depth maps, and extrapolated trajectories for assessing out-of-sequence novel view synthesis.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="wang2025flying" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2025flying_co_stereo.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_wang2025flying"> Flying Co-Stereo: Enabling Long-Range Aerial Dense Mapping via Collaborative Stereo Vision of Dynamic-Baseline </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Zhaoying Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wei Dong<sup>#</sup>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2506.00546</em>
      
      
        
          2025
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2506.00546" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
      [<a href="https://xingxingzuo.github.io/flying_co_stereo" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Lightweight long-range mapping is critical for safe navigation of UAV swarms in large-scale unknown environments. Traditional stereo vision systems with fixed short baselines face limited perception ranges. To address this, we propose Flying Co-Stereo, a cross-agent collaborative stereo vision system that leverages the wide-baseline spatial configuration of two UAVs for long-range dense mapping. Key innovations include: (1) a dual-spectrum visual-inertial-ranging estimator for robust baseline estimation; (2) a hybrid feature association strategy combining deep learning-based cross-agent matching and optical-flow-based intra-agent tracking; (3) A sparse-to-dense depth recovery scheme,refining dense monocular depth predictions using exponential fitting of long-range triangulated sparse landmarks for precise metric-scale mapping. Experiments demonstrate the Flying Co-Stereo system achieves dense 3D mapping up to 70 meters with 2.3%-9.7% relative error, outperforming conventional systems by up to 350% in depth range and 450% in coverage area.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2024ijcv" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2024fmgs.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_zuo2024ijcv">FMGS: Foundation Model Embedded 3D Gaussian
Splatting for Holistic 3D Scene Understanding</a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Pouya Samangouei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yunwen Zhou,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yan Di,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://scholar.google.com/citations?user=DK-ls48AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mingyang Li</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>International Journal of Computer Vision (IJCV)</em>
      
      
        
          2024
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2401.01970" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
      [<a href="https://xingxingzuo.github.io/fmgs" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
      [<a href="https://xingxingzuo.github.io/fmgs" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present Foundation Model Embedded Gaussian Splatting (FMGS), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS).The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by 10.2 percent on open-vocabulary language-based object detection, despite that we are 851X faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. </p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="jens2024ral" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2024nerfvo.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_jens2024ral">NeRF-VO: Real-Time Sparse Visual Odometry With Neural Radiance Fields</a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Jens Naumann,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Binbin Xu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE Robotics and Automation Letters (RA-L)</em>
      
      
        
          2024
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2312.13471" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=El3-hSnuOz0" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
      [<a href="https://xingxingzuo.github.io/nerfvo/" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/jens-nau/NeRF-VO" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We introduce a novel monocular visual odometry (VO) system, NeRF-VO, that integrates learning-based sparse visual odometry for low-latency camera tracking and a neural radiance scene representation for fine-detailed dense reconstruction and novel view synthesis. Our system initializes camera poses using sparse visual odometry and obtains view-dependent dense geometry priors from a monocular prediction network. We harmonize the scale of poses and dense geometry, treating them as supervisory cues to train a neural implicit scene representation. NeRF-VO demonstrates exceptional performance in both photometric and geometric fidelity of the scene representation by jointly optimizing a sliding window of keyframed poses and the underlying dense geometry, which is accomplished through training the radiance field with volume rendering. We surpass SOTA methods in pose estimation accuracy, novel view synthesis fidelity, and dense reconstruction quality across a variety of synthetic and real-world datasets while achieving a higher camera tracking frequency and consuming less GPU memory.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="wu2024cvpr" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2024dynfl.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_wu2024cvpr">Dynamic LiDAR Re-simulation using Compositional Neural Fields</a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Hanfeng Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Or Litany,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Konrad Schindle,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Shengyu Huang<sup>#</sup>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), <b>Highlight</b>, </em>
      
      
        
          2024
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2312.05247" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://shengyuh.github.io/dynfl/assets/Main_videos.m4v" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
      [<a href="https://shengyuh.github.io/dynfl" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/prs-eth/Dynamic-LiDAR-Resimulation" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We introduce DyNFL, a novel neural field-based approach for high-fidelity re-simulation of LiDAR scans in dynamic driving scenes. DyNFL processes LiDAR measurements from dynamic environments, accompanied by bounding boxes of moving objects, to construct an editable neural field. This field, comprising separately reconstructed static backgrounds and dynamic objects, allows users to modify viewpoints, adjust object positions, and seamlessly add or remove objects in the re-simulated scene. A key innovation of our method is the neural field composition technique, which effectively integrates reconstructed neural assets from various scenes through a ray drop test, accounting for occlusions and transparent surfaces. Our evaluation with both synthetic and real-world environments demonstrates that DyNFL substantial improves dynamic scene simulation based on LiDAR scans, offering a combination of physical fidelity and flexible editing capabilities.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="li2024icra" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2024radarcam_depth.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_li2024icra">RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale</a></span>
      <span class="author">
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Han Li,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yukai Ma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yaqing Gu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kewei Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu<sup>#</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
        
          2024
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2401.04325" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://youtu.be/JDn0Sua5d9o" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/MMOCKING/RadarCam-Depth" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We present a novel approach for metric dense depth estimation based on the fusion of a single-view image and a sparse, noisy Radar point cloud. The direct fusion of heterogeneous Radar and image data, or their encodings, tends to yield dense depth maps with significant artifacts, blurred boundaries, and suboptimal accuracy. To circumvent this issue, we learn to augment versatile and robust monocular depth prediction with the dense metric scale induced from sparse and noisy Radar data. We propose a Radar-Camera framework for highly accurate and fine-detailed dense depth estimation with four stages, including monocular depth prediction, global scale alignment of monocular depth with sparse Radar points, quasi-dense scale estimation through learning the association between Radar points and image patches, and local scale refinement of dense depth using a scale map learner. Our proposed method significantly outperforms the state-of-the-art Radar-Camera depth estimation methods by reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2% on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam dataset, respectively.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2023ral" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2023cocolic_b.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lang2023ral"> Coco-LIC: Continuous-Time Tightly-Coupled LiDAR-Inertial-Camera Odometry using Non-Uniform B-spline </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Xiaolei Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chao Chen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kai Tang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yukai Ma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu<sup>#</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em> IEEE Robotics and Automation Letters</em>
      
      
        
          2023
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2309.09808" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=M-vlxK4DWno" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/Coco-LIC" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we propose an efficient continuous-time LiDAR-Inertial-Camera Odometry, utilizing non-uniform B-splines to tightly couple measurements from the LiDAR, IMU, and camera. In contrast to uniform B-spline-based continuous-time methods, our non-uniform B-spline approach offers significant advantages in terms of achieving real-time efficiency and high accuracy. This is accomplished by dynamically and adaptively placing control points, taking into account the varying dynamics of the motion. To enable efficient fusion of heterogeneous LiDAR-Inertial-Camera data within a short sliding-window optimization, we assign depth to visual pixels using corresponding map points from a global LiDAR map, and formulate frame-to-map reprojection factors for the associated pixels in the current image frame. This way circumvents the necessity for depth optimization of visual pixels, which typically entails a lengthy sliding window with numerous control points for continuous-time trajectory estimation. We conduct dedicated experiments on real-world datasets to demonstrate the advantage and efficacy of adopting non-uniform continuous-time trajectory representation. Our LiDAR-Inertial-Camera odometry system is also extensively evaluated on both challenging scenarios with sensor degenerations and large-scale scenarios, and has shown comparable or higher accuracy than the state-of-the-art methods.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="xin2023ismar" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2023simplemapping.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_xin2023ismar">SimpleMapping: Real-Time Visual-Inertial Dense Mapping with Deep Multi-View Stereo</a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Yingye Xin*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*<sup>#</sup></b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Dongyue Lu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em>
      
      
        
          2023
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2306.08648" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=R68sEMhEDNU" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
      [<a href="https://yingyexin.github.io/simplemapping.html" target="_blank" rel="noopener noreferrer">Webpage</a>]
    
    
    
    
    
    
    
    
      [<a href="https://github.com/yingyexin/SimpleMapping" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We present a real-time visual-inertial dense mapping method capable of performing incremental 3D mesh reconstruction with high quality using only sequential monocular images and inertial measurement unit (IMU) readings. 6-DoF camera poses are estimated by a robust feature-based visual-inertial odometry (VIO), which also generates noisy sparse 3D map points as a by-product. We propose a sparse point aided multi-view stereo neural network (SPA-MVSNet) that can effectively leverage the informative but noisy sparse points from the VIO system. The sparse depth from VIO is firstly completed by a single-view depth completion network. This dense depth map, although naturally limited in accuracy, is then used as a prior to guide our MVS network in the cost volume generation and regularization for accurate dense depth prediction. Predicted depth maps of keyframe images by the MVS network are incrementally fused into a global map using TSDF-Fusion. We extensively evaluate both the proposed SPA-MVSNet and the entire visual-inertial dense mapping system on several public datasets as well as our own dataset, demonstrating the system’s impressive generalization capabilities and its ability to deliver high-quality 3D mesh reconstruction online. Our proposed dense mapping system achieves a 39.7% improvement in F-score over existing systems when evaluated on the challenging scenarios of the EuRoC dataset.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2023ram" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/2023fvfusion.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_zuo2023ral">Incremental Dense Reconstruction from Monocular Video with Guided Sparse Feature Volume Fusion </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo</b></em>,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Nan Yang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nathaniel Merrill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Binbin Xu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em> IEEE Robotics and Automation Letters</em>
      
      
        
          2023
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2305.14918" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=bY6zffvbSGE" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Incrementally recovering 3D dense structures from monocular videos is of paramount importance since it enables various robotics and AR applications. Feature volumes have recently been shown to enable efficient and accurate incremental dense reconstruction without the need to first estimate depth, but they are not able to achieve as high of a resolution as depth-based methods due to the large memory consumption of high-resolution feature volumes. This letter proposes a real-time feature volume-based dense reconstruction method that predicts TSDF (Truncated Signed Distance Function) values from a novel sparsified deep feature volume, which is able to achieve higher resolutions than previous feature volume-based methods, and is favorable in outdoor large-scale scenarios where the majority of voxels are empty. An uncertainty-aware multi-view stereo (MVS) network is leveraged to infer initial voxel locations of the physical surface in a sparse feature volume. Then for refining the recovered 3D geometry, deep features are attentively aggregated from multi-view images at potential surface locations, and temporally fused. Besides achieving higher resolutions than before, our method is shown to produce more complete reconstructions with finer detail in many cases. Extensive evaluations on both public and self-collected datasets demonstrate a very competitive real-time reconstruction result for our method compared to state-of-the-art reconstruction methods in both indoor and outdoor settings.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lv2023tmech" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/clic.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lv2023tmech">Continuous-Time Fixed-Lag Smoothing for LiDAR-Inertial-Camera SLAM </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaolei Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jinhong Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mengmeng Wang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu<sup>#</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE/ASME Transactions on Mechatronics</em>
      
      
        
          2023
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2302.07456" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/clic" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Localization and mapping with heterogeneous multisensor fusion have been prevalent in recent years. To adequately fuse multimodal sensor measurements received at different time instants and different frequencies, we estimate the continuous-time trajectory by fixed-lag smoothing within a factor-graph optimization framework. With the continuous-time formulation, we can query poses at any time instants corresponding to the sensor measurements. To bound the computation complexity of the continuous-time fixed-lag smoother, we maintain temporal and keyframe sliding windows with constant size, and probabilistically marginalize out control points of the trajectory and other states, which allows preserving prior information for future sliding-window optimization. Based on continuous-time fixed-lag smoothing, we design tightly coupled multimodal SLAM algorithms with a variety of sensor combinations, like the LiDAR-inertial and LiDAR-inertial-camera SLAM systems, in which online time offset calibration is also naturally supported. More importantly, benefiting from the marginalization and our derived analytical Jacobians for optimization, the proposed continuous-time SLAM systems can achieve real-time performance regardless of the high complexity of continuous-time formulation. The proposed multimodal SLAM systems have been widely evaluated on three public datasets and self-collect datasets. The results demonstrate that the proposed continuous-time SLAM systems can achieve high-accuracy pose estimations and outperform existing state-of-the-art methods.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lang2022ral" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/ctrl_vio.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lang2022ral">Ctrl-VIO: Continuous-Time Visual-Inertial Odometry for Rolling Shutter Cameras </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Xiaolei Lang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Jianxin Huang,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yukai Ma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu<sup>#</sup>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and <em><b>Xingxing</b></em> <em><b>Zuo<sup>#</sup></b></em>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em> IEEE Robotics and Automation Letters</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2208.12008" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/Ctrl-VIO" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this letter, we propose a probabilistic continuous-time visual-inertial odometry (VIO) for rolling shutter cameras. The continuous-time trajectory formulation naturally facilitates the fusion of asynchronized high-frequency IMU data and motion-distorted rolling shutter images. To prevent intractable computation load, the proposed VIO is sliding-window and keyframe-based. We propose to probabilistically marginalize the control points to keep the constant number of keyframes in the sliding window. Furthermore, the line exposure time difference (line delay) of the rolling shutter camera can be online calibrated in our continuous-time VIO. To extensively examine the performance of our continuous-time VIO, experiments are conducted on publicly-available WHU-RSVI, TUM-RSVI, and SenseTime-RSVI rolling shutter datasets. The results demonstrate the proposed continuous-time VIO significantly outperforms the existing state-of-the-art VIO methods.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lv2022tro" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/ob_aware_calib.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_lv2022tro">Observability-Aware Intrinsic and Extrinsic Calibration of LiDAR-IMU Systems </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kewei Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jinhong Xu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Yong Liu
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE Transactions on Robotics</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2205.03276" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/OA-LICalib" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Accurate and reliable sensor calibration is essential to fuse LiDAR and inertial measurements, which are usually available in robotic applications. In this paper, we propose a novel LiDAR-IMU calibration method within the continuous-time batch-optimization framework, where the intrinsics of both sensors and the spatial-temporal extrinsics between sensors are calibrated without using calibration infrastructure such as fiducial tags. Compared to discrete-time approaches, the continuous-time formulation has natural advantages for fusing high rate measurements from LiDAR and IMU sensors. To improve efficiency and address degenerate motions, two observability-aware modules are leveraged: (i) The information-theoretic data selection policy selects only the most informative segments for calibration during data collection, which significantly improves the calibration efficiency by processing only the selected informative segments. (ii) The observability-aware state update mechanism in nonlinear least-squares optimization updates only the identifiable directions in the state space with truncated singular value decomposition (TSVD), which enables accurate calibration results even under degenerate cases where informative data segments are not available. The proposed LiDAR-IMU calibration approach has been validated extensively in both simulated and real-world experiments with different robot platforms, demonstrating its high accuracy and repeatability in commonly-seen human-made environments.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="nate2022cvpr" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/suo_slam.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_nate2022cvpr">Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation</a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Nathaniel Merrill,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yuliang Guo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
            
              
            
              
            
          
          
          
            
              
                
                  Xinyu Huang,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.professoren.tum.de/en/leutenegger-stefan" target="_blank" rel="noopener noreferrer">Stefan Leutenegger</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xi Peng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu Ren,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>
      
      
        
          2022
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2205.01823" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/rpng/suo_slam" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>We propose a keypoint-based object-level SLAM framework that can provide globally consistent 6DoF pose estimates for symmetric and asymmetric objects alike. To the best of our knowledge, our system is among the first to utilize the camera pose information from SLAM to provide prior knowledge for tracking keypoints on symmetric objects – ensuring that new measurements are consistent with the current 3D scene. Moreover, our semantic keypoint network is trained to predict the Gaussian covariance for the keypoints that captures the true error of the prediction, and thus is not only useful as a weight for the residuals in the system’s optimization problems, but also as a means to detect harmful statistical outliers without choosing a manual threshold. Experiments show that our method provides competitive performance to the state of the art in 6DoF object pose estimation, and at a real-time speed. Our code, pre-trained models, and keypoint labels are open-sourced.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2021codevio" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/codevio_overview.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title"><a name="lnk_codevio"> CodeVIO: Visual-inertial odometry with learned optimizable dense depth </a></span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nathaniel Merrill*,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Wei Li,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://people.inf.ethz.ch/pomarc/" target="_blank" rel="noopener noreferrer">Marc Pollefeys</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em><b style="color:red;">Nominated for the "Best Paper Award in Robot Vision (Finalist) of ICRA 2021", </b></em>
        <!-- <br> -->
        <em>IEEE International Conference on Robotics and Automation (ICRA)</em>
      
      
        
          2021
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2012.10133" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=tr_vflFjghY" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this work, we present a lightweight, tightly-coupled deep depth network and visual-inertial odometry (VIO) system, which can provide accurate state estimates and dense depth maps of the immediate surroundings. Leveraging the proposed lightweight Conditional Variational Autoencoder (CVAE) for depth inference and encoding, we provide the network with previously marginalized sparse features from VIO to increase the accuracy of initial depth prediction and generalization capability. The compact encoded depth maps are then updated jointly with navigation states in a sliding window estimator in order to provide the dense local scene geometry. We additionally propose a novel method to obtain the CVAE’s Jacobian which is shown to be more than an order of magnitude faster than previous works, and we additionally leverage First-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous works relying on completely dense residuals, we propose to only provide sparse measurements to update the depth code and show through careful experimentation that our choice of sparse measurements and FEJs can still significantly improve the estimated depth maps. Our full system also exhibits state-of-the-art pose estimation accuracy, and we show that it can run in real-time with single-thread execution while utilizing GPU acceleration only for the network and code Jacobian.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zhang2021pose" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/manifold.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Pose Estimation for Ground Robots: On Manifold Representation, Integration, Reparameterization, and Optimization</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Mingming Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yiming Chen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://scholar.google.com/citations?user=DK-ls48AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mingyang Li</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE Transactions on Robotics</em>
      
      
        
          2021
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1909.03423" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=XGsQPqx2LY4" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this article, we focus on pose estimation dedicated to nonholonomic ground robots with low-cost sensors, by probabilistically fusing measurements from wheel odometers and an exteroceptive sensor. For ground robots, wheel odometers are widely used in pose estimation tasks, especially in applications in planar scenes. However, since wheel odometer only provides two-dimensional (2D) motion measurements, it is extremely challenging to use that for accurate full 6-D pose (3-D position and 3-D orientation) estimation. Traditional methods for 6-D pose estimation with wheel odometers either approximate motion profiles at the cost of accuracy reduction, or rely on other sensors, e.g., inertial measurement unit, to provide complementary measurements. By contrast, we propose a novel motion-manifold-based method for pose estimation of ground robots, which enables to utilize wheel odometers for high-precision 6-D pose estimation. Specifically, the proposed method, first, formulates the motion manifold of ground robots by a parametric representation, second, performs manifold-based 6-D integration with wheel odometer measurements only, and third, reparameterizes manifold representation periodically for error reduction. To demonstrate the effectiveness and applicability of the proposed algorithmic module, we integrate that into a sliding-window pose estimator by using measurements from wheel odometers and a monocular camera. Extensive simulated and real-world experiments are conducted for evaluation, and the proposed algorithm is shown to outperform competing the state-of-the-art algorithms by a significant margin in pose estimation accuracy, especially when deployed in complex, large-scale real-world environments.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lv2021clins" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/clins.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">CLINS: Continuous-Time Trajectory Estimation for LiDAR-Inertial System</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kewei Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jinhong Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiushui Ma,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em><b>Xingxing Zuo</b></em>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        
          2021
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2109.04687" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=7aQJklHg2RM" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/clins" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we propose a highly accurate continuous-time trajectory estimation framework dedicated to SLAM (Simultaneous Localization and Mapping) applications, which enables fuse high-frequency and asynchronous sensor data effectively. We apply the proposed framework in a 3D LiDAR-inertial system for evaluations. The proposed method adopts a non-rigid registration method for continuous-time trajectory estimation and simultaneously removing the motion distortion in LiDAR scans. Additionally, we propose a two-state continuous-time trajectory correction method to efficiently and efficiently tackle the computationally-intractable global optimization problem when loop closure happens. We examine the accuracy of the proposed approach on several publicly available datasets and the data we collected. The experimental results indicate that the proposed method outperforms the discrete-time methods regarding accuracy especially when aggressive motion occurs. Furthermore, we open source our code to benefit research community.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="liu2021mba" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/mba_vo.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">MBA-VO: Motion Blur Aware Visual Odometry</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Peidong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Viktor Larsson,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://people.inf.ethz.ch/pomarc/" target="_blank" rel="noopener noreferrer">Marc Pollefeys</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>International Conference on Computer Vision (ICCV), selected as <b>Oral</b>, </em>
      
      
        
          2021
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2103.13684" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/ethliup/MBA-VO" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Motion blur is one of the major challenges remaining for visual odometry methods. In low-light conditions where longer exposure times are necessary, motion blur can appear even for relatively slow camera motions. In this paper we present a novel hybrid visual odometry pipeline with direct approach that explicitly models and estimates the camera’s local trajectory within the exposure time. This allows us to actively compensate for any motion blur that occurs due to the camera motion. In addition, we also contribute a novel benchmarking dataset for motion blur aware visual odometry. In experiments we show that by directly modeling the image formation process, we are able to improve robustness of the visual odometry, while keeping comparable accuracy as that for images without motion blur.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="Zuo2020IROS" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/lic2_sim.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">LIC-Fusion 2.0: LiDAR-Inertial-Camera Odometry with Sliding-Window Plane-Feature Tracking</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yangyulin.net/" target="_blank" rel="noopener noreferrer">Yulin Yang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Patrick Geneva,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://people.inf.ethz.ch/pomarc/" target="_blank" rel="noopener noreferrer">Marc Pollefeys</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        
          2020
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2008.07196" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=waE5nepxD-Q" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=waE5nepxD-Q" target="_blank" rel="noopener noreferrer">Video2</a>]
    
    
    
    
    
      [<a href="https://XingxingZuo.github.io/assets/documents/tr_lic2.pdf" target="_blank">Supp</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Multi-sensor fusion of multi-modal measurements from commodity inertial, visual and LiDAR sensors to provide robust and accurate 6DOF pose estimation holds great potential in robotics and beyond. In this paper, building upon our prior work (i.e., LIC-Fusion), we develop a sliding-window filter based LiDAR-Inertial-Camera odometry with online spatiotemporal calibration (i.e., LIC-Fusion 2.0), which introduces a novel sliding-window plane-feature tracking for efficiently processing 3D LiDAR point clouds. In particular, after motion compensation for LiDAR points by leveraging IMU data, low-curvature planar points are extracted and tracked across the sliding window. A novel outlier rejection criterion is proposed in the plane-feature tracking for high-quality data association. Only the tracked planar points belonging to the same plane will be used for plane initialization, which makes the plane extraction efficient and robust. Moreover, we perform the observability analysis for the LiDAR-IMU subsystem and report the degenerate cases for spatiotemporal calibration using plane features. While the estimation consistency and identified degenerate motions are validated in Monte-Carlo simulations, different real-world experiments are also conducted to show that the proposed LIC-Fusion 2.0 outperforms its predecessor and other state-of-the-art methods.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2020multimodal" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/jfr_multimodal.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Multimodal localization: Stereo over LiDAR map</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wenlong Ye,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yangyulin.net/" target="_blank" rel="noopener noreferrer">Yulin Yang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Renjie Zheng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Teresa Vidal-Calleja,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Yong Liu
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of Field Robotics</em>
      
      
        
          2020
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we present a real-time high-precision visual localization system for an autonomous vehicle which employs only low-cost stereo cameras to localize the vehicle with a priori map built using a more expensive 3D LiDAR sensor. To this end, we construct two different visual maps: a sparse feature visual map for visual odometry (VO) based motion tracking, and a semidense visual map for registration with the prior LiDAR map. To register two point clouds sourced from different modalities (i.e., cameras and LiDAR), we leverage probabilistic weighted normal distributions transformation (ProW-NDT), by particularly taking into account the uncertainty of source point clouds. The registration results are then fused via pose graph optimization to correct the VO drift. Moreover, surfels extracted from the prior LiDAR map are used to refine the sparse 3D visual features that will further improve VO-based motion estimation. The proposed system has been tested extensively in both simulated and real-world experiments, showing that robust, high-precision, real-time localization can be achieved.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="lv2020targetless" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/LIcalib.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Targetless calibration of lidar-imu system based on continuous-time batch estimation</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  Jiajun Lv,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jinhong Xu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kewei Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                and <em><b>Xingxing Zuo</b></em>
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        
          2020
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/2007.14759" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=umXgc4bfhy0" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
      [<a href="https://github.com/APRIL-ZJU/lidar_IMU_calib" target="_blank" rel="noopener noreferrer">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Sensor calibration is the fundamental block for a multi-sensor fusion system. This paper presents an accurate and repeatable LiDAR-IMU calibration method (termed LI-Calib), to calibrate the 6-DOF extrinsic transformation between the 3D LiDAR and the Inertial Measurement Unit (IMU). % Regarding the high data capture rate for LiDAR and IMU sensors, LI-Calib adopts a continuous-time trajectory formulation based on B-Spline, which is more suitable for fusing high-rate or asynchronous measurements than discrete-time based approaches. % Additionally, LI-Calib decomposes the space into cells and identifies the planar segments for data association, which renders the calibration problem well-constrained in usual scenarios without any artificial targets. We validate the proposed calibration approach on both simulated and real-world experiments. The results demonstrate the high accuracy and good repeatability of the proposed method in common human-made scenarios. To benefit the research community, the code of this useful toolbox is also open-source.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography">
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="Zuo2019IROS" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/lic1_overview_b.gif">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">LIC-Fusion: LiDAR-Inertial-Camera Odometry</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Patrick Geneva,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Woosik Lee,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems</em>
      
      
        
          2019
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1909.04102" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=z8WG4miBk3s" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents a tightly-coupled multi-sensor fusion algorithm termed LiDAR-inertial-camera fusion (LIC-Fusion), which efficiently fuses IMU measurements, sparse visual features, and extracted LiDAR points. In particular, the proposed LIC-Fusion performs online spatial and temporal sensor calibration between all three asynchronous sensors, in order to compensate for possible calibration variations. The key contribution is the optimal (up to linearization errors) multi-modal sensor fusion of detected and tracked sparse edge/surf feature points from LiDAR scans within an efficient MSCKF-based framework, alongside sparse visual feature observations and IMU readings. We perform extensive experiments in both indoor and outdoor environments, showing that the proposed LIC-Fusion outperforms the state-of-the-art visual-inertial odometry (VIO) and LiDAR odometry methods in terms of estimation accuracy and robustness to aggressive motions.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="Zuo2019ISRR" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/icr.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Visual-Inertial Localization for Skid-Steering Robots with Kinematic Constraints</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mingming Zhang*,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yiming Chen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://scholar.google.com/citations?user=DK-ls48AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Mingyang Li</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>International Symposium on Robotics Research (ISRR)</em>
      
      
        
          2019
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1911.05787" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
    
    
    
    
    
      [<a href="https://XingxingZuo.github.io/assets/documents/tr_icr.pdf" target="_blank">Supp</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>While visual localization or SLAM has witnessed great progress in past decades, when deploying it on a mobile robot in practice, few works have explicitly considered the kinematic (or dynamic) constraints of the real robotic system when designing state estimators. To promote the practical deployment of current state-of-the-art visual-inertial localization algorithms, in this work we propose a low-cost kinematics-constrained localization system particularly for a skid-steering mobile robot. In particular, we derive in a principle way the robot’s kinematic constraints based on the instantaneous centers of rotation (ICR) model and integrate them in a tightly-coupled manner into the sliding-window bundle adjustment (BA)-based visual-inertial estimator. Because the ICR model parameters are time-varying due to, for example, track-to-terrain interaction and terrain roughness, we estimate these kinematic parameters online along with the navigation state. To this end, we perform in-depth the observability analysis and identify motion conditions under which the state/parameter estimation is viable. The proposed kinematics-constrained visual-inertial localization system has been validated extensively in different terrain scenarios.</p>
    </span>
    
  </div>
</div>
</li>
<li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2019visual" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/mapreuse1.webp">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Visual-Inertial Localization With Prior LiDAR Map Constraints</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                
                  <em><b>Xingxing</b></em> <em><b>Zuo*</b></em>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Patrick Geneva*,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://yangyulin.net/" target="_blank" rel="noopener noreferrer">Yulin Yang</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Wenlong Ye,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>IEEE Robotics and Automation Letters</em>
      
      
        
          2019
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
    
      [<a href="https://www.youtube.com/watch?v=76fXMVhhX4E&amp;t=1s" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this letter, we develop a low-cost stereo visual-inertial localization system, which leverages efficient multi-state constraint Kalman filter (MSCKF)-based visual-inertial odometry (VIO) while utilizing an a priori LiDAR map to provide bounded-error three-dimensional navigation. Besides the standard sparse visual feature measurements used in VIO, the global registrations of visual semi-dense clouds to the prior LiDAR map are also exploited in a tightly-coupled MSCKF update, thus correcting accumulated drift. This cross-modality constraint between visual and LiDAR pointclouds is particularly addressed. The proposed approach is validated on both Monte Carlo simulations and real-world experiments, showing that LiDAR map constraints between clouds created through different sensing modalities greatly improve the standard VIO and provide bounded-error performance.</p>
    </span>
    
  </div>
</div>
</li>
</ol>
    
      <ol class="bibliography"></ol>
    
      <ol class="bibliography"><li>
<!-- Refer to https://github.com/alshedivat/al-folio/blob/master/_layouts/bib.html
and 
https://github.com/Mayankm96/Mayankm96.github.io/blob/source/_layouts/bib.html -->
<div id="zuo2017robust" class="col three">
  <div style="clear: both;">
    <div style="">
        <img class="col bibone first" src="https://XingxingZuo.github.io/assets/img/teaser/plslam_overview.png">
    </div>
  </div>
  <div class="col bibtwo last">
    
      <span class="title">Robust visual SLAM with point and line features</span>
      <span class="author">
        
          
          
          
          
          
          
            
              
                <em><b>Xingxing Zuo</b></em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaojia Xie,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yong Liu,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="http://udel.edu/~ghuang/" target="_blank" rel="noopener noreferrer">Guoquan Huang</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em></em>
        <!-- <br> -->
        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
      
      
        
          2017
        
      
      </span>
    

    <span class="links">
    
      [<a class="abstract" style="color: $pub-theme-color">Abs</a>]
    
    
      [<a href="http://arxiv.org/abs/1711.08654" target="_blank" rel="noopener noreferrer">arXiv</a>]
    
    
      [<a href="https://www.youtube.com/watch?v=qK8vgY1Gjgg" target="_blank" rel="noopener noreferrer">Video</a>]
    
    
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper, we develop a robust efficient visual SLAM system that utilizes heterogeneous point and line features. By leveraging ORB-SLAM [1], the proposed system consists of stereo matching, frame tracking, local mapping, loop detection, and bundle adjustment of both point and line features. In particular, as the main theoretical contributions of this paper, we, for the first time, employ the orthonormal representation as the minimal parameterization to model line features along with point features in visual SLAM and analytically derive the Jacobians of the re-projection errors with respect to the line parameters, which significantly improves the SLAM solution. The proposed SLAM has been extensively tested in both synthetic and real-world experiments whose results demonstrate that the proposed system outperforms the state-of-the-art methods in various scenarios.</p>
    </span>
    
  </div>
</div>
</li></ol>
    
    </div>
  </article>

  <br>
  <br>
  <br>  
  <object>
    <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=1twVP4MxlSt5qz7WFcQ2STz9_P7EHXjk87_cAwV5zq4&amp;cl=ffffff&amp;w=a"></script>
  </object>
  <br>
<!--   <center>
    <object>
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=1twVP4MxlSt5qz7WFcQ2STz9_P7EHXjk87_cAwV5zq4"></script>
    </object>
  </center> -->

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2026 Xingxing  Zuo.
    Powered by <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
